{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi driver V3\n",
    "QLearning algorith\n",
    "\n",
    "By HERTZ Arthur, LUCAS Michaël, TROUSSET Alexis, ROUSSEL Rémi, BRILLARD Quentin.\n",
    "\n",
    "## 1. What is [Taxi V3](https://gym.openai.com/envs/Taxi-v3/)\n",
    "\n",
    "Taxi V3 is  gym environment that simulate A taxi inside a city. The environment can be illutrate by the following grid wich can be shown by the code bellow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Grid explanation\n",
    "This grid above represent the city. \n",
    "On it we have for points, that are shown by letters (R,G,Y,B) that represent locations where we can pickup clients and drop them off. Each '|' represent wall taht the taxi can't pass through. The yellow rectangle represent the taxi, this rectangle is yellow when the taxi is free and green is when we have a client.\n",
    "\n",
    "### 1.2 Reward notion\n",
    "This environment give us a reward after each action that we send to gym. We have three possible reward:\n",
    "- -1 pts Is for each action that does'nt have good or bad result\n",
    "- -10 pts Is for each bad actions (try to pick up a client while we cant, or drop him off while we aren't on a good location)\n",
    "- +20 pts For each successful drop-off\n",
    "\n",
    "### 1.3 Action list\n",
    "\n",
    "To resolve this \"game\" we can make six different actions:\n",
    "- move south\n",
    "- move north\n",
    "- move est\n",
    "- move west\n",
    "- pickup\n",
    "- dropdown\n",
    "\n",
    "## 2 Reinforcement Learning\n",
    "\n",
    "In reinforcement learning the agent encoured a state. With this state the agent have to takes actions according to the state. \n",
    "So to summarize our agent for a given state, for an action it obtains a certain result and we have to keep in memory each result from each actions from each states. So to keep all the result in memory we have to build a table.\n",
    "\n",
    "### 2.1 Build the QTable\n",
    "\n",
    "To keep track of all the results we need to build a table that summarizes the different actions on the x-axis (here the directional shifts and the action of picking up and dropping off a customer) and on the y-axis the states of the Taxi, i.e. where it is in the city.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "q_table = numpy.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After init the qtable, we have to define all Hyperparameters:\n",
    "- the alpha, the learning rate\n",
    "- the gamma, the discount rate\n",
    "- the epsilon, define the chance to select a random action (exploration) insteadof maximising reward (exploitation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # Learning Rate\n",
    "gamma = 0.6  # Discount Rate\n",
    "epsilon = 0.1  # Chance of selecting a random action instead of maximising reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exploration and exploitation\n",
    "\n",
    "At the beginig we must know more about the environment (what reward we will get from an action made from a given state), to fil our qlearning table. A little at a time it is preferable to exploit all results to get best rewards. So the goal from the espilon variable is to at the begining maximized chance to do exploration and at the end maximzed chance to do exploitation. \n",
    "\n",
    "## 3 Lets starts !\n",
    "\n",
    "### 3.1 Reset the environment\n",
    "\n",
    "After import and select the good gym environment (CF part 1), we have to reset the environment, define variable to get some metrics and define number of episodes for training and for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "training_episodes = 20000  # Amount of times to run environment while training.\n",
    "display_episodes = 10  # Amount of times to run environment after training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.2 Training the agent\n",
    "  Now we are in the training step. The purpose from this part is to train our agent to take best descision in the future.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(training_episodes):\n",
    "    \n",
    "    # Get the active state\n",
    "    state = env.reset()\n",
    "    # Define that we doesn't finished the episode\n",
    "    done = False\n",
    "    # Set penalities and rewards to 0\n",
    "    penalties, reward = 0, 0\n",
    "    \n",
    "    # loop while we doesn't finish the current episode\n",
    "    while not done:\n",
    "        # Make a randome test if we have to do exploitation or exploration\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            # Make an exploration action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = numpy.argmax(q_table[state])\n",
    "        \n",
    "        # Execute the next step\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Retrieve old value from the q-table.\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = numpy.max(q_table[next_state])\n",
    "        \n",
    "        # Update q-value for the currentstate\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        \n",
    "        # Check if we get a penalties\n",
    "        if reward == -10:\n",
    "            # Increment penalties number\n",
    "            penalties += 1\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "print(\"Training finished.\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Solve the game\n",
    "After training the agent we are able to appy an algorithme to solve the game. But first we have to define two varaibles to get somme statistics about the algorithm execution. It is the total epochs and total penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs, total_penalties = 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able toloop and display result for the following algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 130\n",
      "State: 410\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "frames = [] # for animation\n",
    "for _ in range(display_episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = numpy.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        \n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Get the result\n",
    "\n",
    "Now after algorithme execution we are able to display the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 10 episodes:\n",
      "Average timesteps per episode: 13.0\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Results after {display_episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / display_episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / display_episodes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T-AIA902",
   "language": "python",
   "name": "t-aia902"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
